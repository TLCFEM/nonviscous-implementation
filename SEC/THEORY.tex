\section{Background}
\subsection{Nonviscously Damped System}
Without loss of generality, consider the equation of motion of a nonviscously damped inelastic multi-degree-of-freedom (MDOF) system,
\begin{gather}\label{eq:eom}
\mb{y}+\bbf=\mb{p},
\end{gather}
where $\mb{y}=\mb{y}\left(\bu,\bv,\ba\right)$ is the resistance vector of the system, $\mb{p}=\mb{p}\left(t\right)$ is the external load vector, $\bu=\bu\left(t\right)$, $\bv=\bv\left(t\right)=\dot{\bu}$ and $\ba=\ba\left(t\right)=\dot{\bv}$ are the displacement, velocity and acceleration vectors, and $\bbf$ is the nonviscous damping force which can be expressed in the form of the convolution of the kernel $g=g\left(t\right)$ and the vector $\bw$, viz., $\bbf\left(t\right)=\left(g*\bw\right)\left(t\right)$.

Note here, $\bw$ can be either the exact velocity vector $\bv$, or a vector that depends on $\bv$ (e.g., a subset of $\bv$). This is beneficial when it comes to compositing flexible damping which will be discussed later in this work. Formally, it can be represented by
\begin{gather}
\bw=\mb{T}\bv,
\end{gather}
where $\mb{T}$ picks the participating DoFs, and can be, for example, a square diagonal matrix with its diagonal entries being either one or zero.

Since it is an inelastic system, the stiffness matrix $\mb{K}$, the viscous damping matrix $\mb{C}$ and the mass matrix $\mb{M}$ are
\begin{gather}
\pdfrac{\mb{y}}{\bu}=\mb{K},\qquad
\pdfrac{\mb{y}}{\bv}=\mb{C},\qquad
\pdfrac{\mb{y}}{\ba}=\mb{M}.
\end{gather}
The viscous damping matrix $\mb{C}$ may not be trivial as the system may consist of viscous damping components (e.g., viscous damper devices). Using $\bu$ as the basic quantity, the effective stiffness matrix $\bbar{K}$
\begin{gather}
\bbar{K}=\ddfrac{\mb{y}}{\bu}=\mb{K}+\mb{C}\ddfrac{\bv}{\bu}+\mb{M}\ddfrac{\ba}{\bu}
\end{gather}
is the combination of the three, its specific form depends on the specific time integration method used.
\section{Nonviscous Damping With A Single Exponential Kernel}\label{sec:single}
The present section introduces an enhancement of the algorithm proposed by \citet{Adhikari2004} through optimisation. The improvement is accomplished by reformulating the algorithm into a form that enables the integration of arbitrary kernel functions.
\subsection{A Single Exponential Kernel}
We start with the scalar-valued exponential kernel function
\begin{gather}
g=g\left(t\right)=m\exp\left(-st\right),
\end{gather}
where $s$ is often denoted by the relaxation parameter $\mu$, $m$ is often denoted by $c\mu$ in which $c$ is the damping constant. In this work, $s$ and $m$ are adopted for brevity.
The convolution can be then expressed as
\begin{gather}\label{eq:single_conv}
\bbf\left(t\right)=\left(g*\bw\right)\left(t\right)=\int_0^tg(t-\tau)\cdot\bw\left(\tau\right)~\md{\tau}=\int_0^tm\exp\left(-s\left(t-\tau\right)\right)\cdot\bw\left(\tau\right)~\md{\tau}.
\end{gather}
\eqsref{eq:single_conv} corresponds to the solution of the following ODE \citep[see, e.g.,][\S~80]{Zwillinger2021},
\begin{gather}\label{eq:single_conv_ode}
\dot{\bbf}=-s\bbf+m\bw.
\end{gather}
It can be validated by solving \eqsref{eq:single_conv_ode} with the assistance of the integrating factor $\exp\left(st\right)$. Here it is assumed that $\bbf\left(0\right)=\mb{0}$.
\subsection{An Efficient Direct Time Integration Algorithm}
Instead of directly integrating \eqsref{eq:single_conv} using higher-order methods (such as the Runge--Kutta family), to develop an efficient algorithm, \eqsref{eq:single_conv_ode} can be combined with \eqsref{eq:eom} such that the system becomes
\begin{gather}\label{eq:eqv_sys}
\left\{
\begin{array}{l}
\dot{\bbf}=-s\bbf+m\bw,\\
\mb{y}+\bbf=\mb{p},
\end{array}\right.
\end{gather}
in which the first equation is a first-order ODE while the second equation is a second-order ODE.

Given that popular time integration methods are of second-order accuracy, in the context of a discretised iterative solving schema, \eqsref{eq:single_conv_ode} can be rewritten as follows using the trapezoidal rule, which is second-order accurate,
\begin{gather}
\bbf_{n+1}=\bbf_n+\dfrac{\Delta{}t}{2}\left(\dot{\bbf}_n+\dot{\bbf}_{n+1}\right).
\end{gather}
Expanding and rearranging lead to
\begin{gather}\label{eq:discretised_c}
\bbf_{n+1}=\dfrac{2-s\Delta{}t}{2+s\Delta{}t}\bbf_n+\dfrac{m\Delta{}t}{2+s\Delta{}t}\bw_n+\dfrac{m\Delta{}t}{2+s\Delta{}t}\bw_{n+1},
\end{gather}
in which subscripts $\left(\cdot\right)_{n+1}$ and $\left(\cdot\right)_n$ denote the corresponding quantity at $t_n$ and $t_{n+1}=t_n+\Delta{}t$.

Assuming the Newmark method is used for \eqsref{eq:eom}, viz., \eqsref{eq:eom} is satisfied at $t_{n+1}$, then, accounting for \eqsref{eq:discretised_c}, \eqsref{eq:eom} is
\begin{gather}\label{eq:residual}
\mb{y}_{n+1}+\dfrac{2-s\Delta{}t}{2+s\Delta{}t}\bbf_n+\dfrac{m\Delta{}t}{2+s\Delta{}t}\bw_n+\dfrac{m\Delta{}t}{2+s\Delta{}t}\bw_{n+1}=\mb{p}_{n+1}.
\end{gather}
Differentiation leads to the following revised effective stiffness $\bhat{K}_{n+1}$,
\begin{gather}\label{eq:revised_k}
\bhat{K}_{n+1}=\bbar{K}_{n+1}+\dfrac{m\Delta{}t}{2+s\Delta{}t}\mb{T}\ddfrac{\bv_{n+1}}{\bu_{n+1}}.
\end{gather}
Typically, $\ddfrac{\bv}{\bu}$ reduces to a scalar constant (multiplied by an identity matrix), for example, in the Newmark method, it is $\dfrac{\gamma}{\beta\Delta{}t}$.
\begin{Objective}
\paragraph{Remark}
If other direct time integration methods are used for \eqsref{eq:eom}, other methods can be applied to \eqsref{eq:single_conv_ode}. The following are some examples.
\begin{enumerate}
\item For the Bathe two-step method \citep{Noh2019}, the TR-BDF2 method \citep{Bank1985} can be used.
\item For the OALTS method \citep{Zhang2021}, the BDF2 method can be used.
\item For the generalised-$\alpha$ method \citep{Chung1993} and the GSSSS method \citep{Zhou2003}, the variable step size BDF2 method can be used.
\end{enumerate}
\end{Objective}

By introducing the revised resistance $\bhat{y}_{n+1}$ as
\begin{gather}\label{eq:revised_y}
\bhat{y}_{n+1}=\mb{y}_{n+1}+\dfrac{2-s\Delta{}t}{2+s\Delta{}t}\bbf_n+\dfrac{m\Delta{}t}{2+s\Delta{}t}\bw_n+\dfrac{m\Delta{}t}{2+s\Delta{}t}\bw_{n+1},
\end{gather}
the linearised system to be solved is
\begin{gather}\label{eq:revised_system}
\bhat{K}_{n+1}\delta\bu=\mb{p}_{n+1}-\bhat{y}_{n+1}.
\end{gather}

The local iteration body is summarised in \algoref{algo:single_model} with subscript $\left(\cdot\right)_{n+1}$ dropped for brevity.
The steps with a leading \faMicrochip~symbol augment global effective stiffness and resistance to obtain $\bhat{K}$ and $\bhat{y}$. Those are additional steps that need to be computed compared to a conventional algorithm for viscously damped systems. Once convergence is achieved, it is necessary to store the history of nonviscous damping force $\bbf_n\leftarrow\bbf$.
\begin{breakablealgorithm}
\setstretch{1.8}
\caption{iteration body of solving nonviscously damped system with one exponential kernel}\label{algo:single_model}
\begin{algorithmic}
\State \textbf{Input}: $\bbar{K}$, $\bu$, $\bv$, $\mb{y}$, $\mb{p}$ (quantities obtained via conventional manner as if there is no nonviscous damping) and $\bbf_n$
\State \textbf{Output}: $\bu$
\State compute $\bw$ from $\bv$
\State \faMicrochip~compute nonviscous damping force $\bbf=\dfrac{2-s\Delta{}t}{2+s\Delta{}t}\bbf_n+\dfrac{m\Delta{}t}{2+s\Delta{}t}\bw_n+\dfrac{m\Delta{}t}{2+s\Delta{}t}\bw$\Comment{\eqsref{eq:discretised_c}}
\State \faMicrochip~compute revised stiffness $\bhat{K}=\bbar{K}+\dfrac{m\Delta{}t}{2+s\Delta{}t}\mb{T}\ddfrac{\bv}{\bu}$\Comment{\eqsref{eq:revised_k}}
\State \faMicrochip~compute revised resistance $\bhat{y}=\mb{y}+\bbf$\Comment{\eqsref{eq:revised_y}}
\State $\delta\bu=\bhat{K}^{-1}\left(\mb{p}-\bhat{y}\right)$\Comment{\eqsref{eq:revised_system}}
\State update and return $\bu\leftarrow\bu+\delta\bu$
\end{algorithmic}
\end{breakablealgorithm}

Upon careful comparison, it becomes evident that the algorithm presented bears resemblance to the one discussed by \citet{Adhikari2004}. However, there are differences between them. Specifically, \algoref{algo:single_model} tracks the history of nonviscous damping force, and it does not differentiate between full-rank and rank-deficient cases. As a result, it eliminates the need for any supplementary matrix factorisations. \algoref{algo:single_model} directly discretises the EOM without converting it to a first-order system via the state space.

Unlike other algorithms, such as the one by \citet{Cortes2009}, \algoref{algo:single_model} does not impose additional requirements on the time integration method used. Some existing state space methods manage to eliminate the convolution integral term from the \textbf{continuous} version of \eqsref{eq:eom} \citep[see][]{Wagner2003,Wu2019}, which, in the writers' opinion, overcomplicates the solution procedure in the context of numerical analysis, given that the analytical first-order ODE in the state space would still need to be discretised \citep{Adhikari2004} and numerically integrated for general systems. Nonetheless, whenever analytical solutions are sought, those methods may provide extra merits that direct time integration methods do not offer, at the cost of increasing computational cost.
\subsection{Complexity Analysis}
The term $\mb{T}\ddfrac{\bv}{\bu}\delta\bu$ can be efficiently computed by masking $\delta\bu$ if $\mb{T}$ is a diagonal matrix while $\ddfrac{\bv}{\bu}$ can be equivalently converted into a scalar (see discussion before). Assuming the system has a size of $n$, it requires at most $n$ floating point number arithmetic. All three extra steps require pure vector operations, the total number of floating point number multiplications is $4n$, that is a time complexity of $\mathcal{O}\left(n\right)$.

\algoref{algo:single_model} requires no memory reallocation, the additional storage needed is for the nonviscous damping forces $\bbf_n$, implying a space complexity of $\mathcal{O}\left(n\right)$.
\section{Nonviscous Damping With Arbitrary Kernel}\label{sec:arbitrary}
\algoref{algo:single_model} alone does not enable adoption of arbitrary kernel functions, thus, has limited applicability. In this section, we present the strategy of modelling arbitrary kernels using \algoref{algo:single_model} as the foundation.
\subsection{Sum of Exponentials}
Now consider, instead of a single exponential function, multiple exponential functions such that
\begin{gather}\label{eq:sum_exp}
g=\sum_{l=1}^{j}g_l\left(t\right)=\sum_{l=1}^{j}m_l\exp\left(-s_lt\right),
\end{gather}
where $m_l$ and $s_l$ can now be complex numbers, then
\begin{gather}\label{eq:sum_conv}
\bbf=g*\bw=\sum_{l=1}^{j}g_l*\bw=\sum_{l=1}^{j}\bbf_l.
\end{gather}
For each $\bbf_l$, \eqsref{eq:discretised_c} also holds and only involves $\bbf_l$ itself and the common quantity $\bw$. Thus,
\begin{gather}
\bbf_l=\dfrac{2-s_l\Delta{}t}{2+s_l\Delta{}t}\bbf_{l,n}+\dfrac{m_l\Delta{}t}{2+s_l\Delta{}t}\bw_n+\dfrac{m_l\Delta{}t}{2+s_l\Delta{}t}\bw.
\end{gather}

Similar to the single function case, substituting $\bbf_l$ into \eqsref{eq:eom}, differentiation yields the revised stiffness
\begin{gather}\label{eq:revised_k_multi}
\bhat{K}=\bbar{K}+\sum_{l=1}^{j}\dfrac{m_l\Delta{}t}{2+s_l\Delta{}t}\mb{T}\ddfrac{\bv}{\bu},
\end{gather}
and the revised resistance
\begin{gather}\label{eq:revised_y_multi}
\bhat{y}=\mb{y}+\sum_{l=1}^{j}\dfrac{2-s_l\Delta{}t}{2+s_l\Delta{}t}\bbf_{l,n}+\sum_{l=1}^{j}\dfrac{m_l\Delta{}t}{2+s_l\Delta{}t}\bw_n+\sum_{l=1}^{j}\dfrac{m_l\Delta{}t}{2+s_l\Delta{}t}\bw.
\end{gather}

Noting that within each sum, the operations performed are identical to that in the single function case, the complexity, in this case, is $\mathcal{O}\left(jn\right)$ for both time and space. As long as parameters $m_l$ and $s_l$ are real or pairs of complex conjugates, $\bbf$ is guaranteed to be real.
\subsection{Damping Composition}
It is possible to assign different kernels to different subsets of the velocity vector. Formally, instead of \eqsref{eq:sum_conv},
\begin{gather}
\bbf=\sum_{k=1}^{i}g^k*\bw^k,
\end{gather}
where $g^k$ is the kernel applied to $\bw^k$, if $g^k$ can be expressed as the sum of exponentials,
\begin{gather}
g^k=\sum_{l=1}^{j^k}g_l^k\left(t\right)=\sum_{l=1}^{j^k}m_l^k\exp\left(-s_l^kt\right),
\end{gather}
in its explicit form,
\begin{gather}\label{eq:composition}
\bbf=\sum_{k=1}^{i}\sum_{l=1}^{j^k}\bbf_l^k=\sum_{k=1}^{i}\sum_{l=1}^{j^k}g_l^k\left(t\right)\bw^k=\sum_{k=1}^{i}\sum_{l=1}^{j^k}m_l^k\exp\left(-s_l^kt\right)\bw^k.
\end{gather}

Since additivity still holds, there is no essential difference between \eqsref{eq:sum_conv} and \eqsref{eq:composition}, in which $m_l^k$ and $s_l^k$ are the parameters for the $l$-th component of the $k$-th kernel, $\bw^k=\mb{T}^k\bv$ can be obtained by either node-based or element-based rules. For the former, it is assumed different regions (characterised by nodes) possess different damping responses. For the latter, it is assumed different elements possess different damping responses, similar to a typical assembly process, example applications can be seen elsewhere \citep{Friswell2007}. No matter how $\bw^k$ is constructed, the revised stiffness and resistance for each $\bbf_l^k$ only require vector--scalar operations (while $\bw^k$ itself may be computed based on $\bv$ via matrix--vector operations).

Denoting
\begin{gather}
j_\text{max}=\max_{k\in\{1,2,\cdots,i\}}\left(j^k\right),
\end{gather}
the time and space complexity is $\mathcal{O}\left(ij_\text{max}n\right)$.
\subsection{Arbitrary Kernel}
Sum of exponentials is able to provide a wide coverage of various kernel functions \citep[c.f.,][]{Adhikari2003}, as a decent amount of kernels can be equivalently expressed as sums of exponentials.

It is clear that the Fourier transform (exponential form) allows one to express arbitrary smooth function $g\left(t\right)$ as an infinite sum of exponentials, even if $g\left(t\right)$ is non-periodic. It is possible to approximate $g\left(t\right)$ if a fast converging exponential series can be found.
One can use Prony's method \citep[see, e.g.,][]{Hamming1987} and its derivations \citep{Hokanson2013} to find a proper approximation. However, it tends to have a slow convergence \citep{Trudnowski1999} which inevitably leads to a large number of exponentials that would impair computational efficiency. By further adopting model reduction, \citet{Gao2022} presented a method, named as VPMR, with controllable magnitudes of exponents and fast convergence, to compute the desired approximation, formally,
\begin{gather}
\max_{t\in{}I}{\abs{g\left(t\right)-\sum_jm_j\exp\left(-s_jt\right)}}<\epsilon,
\end{gather}
where $I$ is a finite interval that could be an arbitrary subset of $\mathbb{R}^+$, $\epsilon$ is the error tolerance.

By using VPMR, it is feasible to convert nonviscous damping with arbitrary (in terms of both number and form) kernels applied to the dynamic system into the form of \eqsref{eq:composition}. Noting that $\epsilon$ is a user input, by assigning a tolerance close to (or less than) machine epsilon --- around \num{e-16} for double precision floating point representation, an accurate equivalence of arbitrary kernel function can be obtained for nonviscous damping computation. In practice, such a tolerance only needs to be smaller than the analysis tolerance.

It is worth emphasising that converting the desired kernel functions to sums of exponentials a) is independent of the dynamic system of interest and b) is performed offline, viz., ahead of time. Combining with \algoref{algo:single_model}, the following procedure can be employed to model nonviscously damped systems with arbitrary kernels.
\begin{Objective}
\begin{enumerate}
\item Determine number and form of kernels $g^k$ to be used.
\item Approximate time step size $\Delta{}t$ that would be used.
\item Use the VPMR algorithm \citep{Gao2022} to find approximations of kernels with tolerance set to $\Delta{}t^2$ such that for each $k$,
\begin{gather}
\abs{g^k-\sum_lg^k_l}<\Delta{}t^2.
\end{gather}
\item Formulate damping matrix and resistance using exponentials found by VPMR for each kernel. Each kernel would be applied to selected nodes/elements.
\item Use \algoref{algo:single_model} with proper time integration method to solve the system.
\end{enumerate}
\end{Objective}